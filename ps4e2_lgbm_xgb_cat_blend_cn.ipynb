{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 68479,
          "databundleVersionId": 7609535,
          "sourceType": "competition"
        },
        {
          "sourceId": 7009925,
          "sourceType": "datasetVersion",
          "datasetId": 4030196
        }
      ],
      "dockerImageVersionId": 30646,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "pss4e2 study note",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "5vGVHTm2yM9D"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "playground_series_s4e2_path = kagglehub.competition_download('playground-series-s4e2')\n",
        "aravindpcoder_obesity_or_cvd_risk_classifyregressorcluster_path = kagglehub.dataset_download('aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "BWWGzoUuyM9F"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Created by Cosmic Black\n",
        "\n",
        "#### This notebook is studied from <a href=\"https://www.kaggle.com/code/ksevta/ps4e2-xgb-lgbm-0-92#Feature-Engineering-&-Processing\">PS4E2: XGB / LGBM | 0.92+</a>. If you think my notebook is useful, don't forget to share it with all."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "8GW6_etqyM9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "72NpRRmfyM9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the notebook by importing essential libraries for data handling, text processing, dimensionality reduction, random number generation, and warning management"
      ],
      "metadata": {
        "id": "vrVORvYq2gxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#necessary\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') #to suppress warning messages"
      ],
      "metadata": {
        "trusted": true,
        "id": "bC6gSKMmyM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "vCmiURFhyM9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#config\n",
        "class Config():\n",
        "    origin_path=\"/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv\"\n",
        "    train_path='/kaggle/input/playground-series-s4e2/train.csv'\n",
        "    test_path='/kaggle/input/playground-series-s4e2/test.csv'\n",
        "    submission_path='/kaggle/input/playground-series-s4e2/sample_submission.csv'\n",
        "    seed=2024\n",
        "    num_folds=10\n",
        "    TARGET_NAME ='NObeyesdad'"
      ],
      "metadata": {
        "trusted": true,
        "id": "DCF0GbaXyM9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessor"
      ],
      "metadata": {
        "id": "hxLv_en-yM9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self,train_path=None,origin_path=None,test_path=None,TARGET_NAME ='target',seed=2024):\n",
        "        self.seed=seed\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "        random.seed(self.seed)\n",
        "\n",
        "        self.TARGET_NAME=TARGET_NAME\n",
        "\n",
        "        if train_path!=None:\n",
        "            self.train_df=pd.read_csv(train_path)\n",
        "            self.train_df.drop(['id'],axis=1,inplace=True)\n",
        "        else:\n",
        "            assert 0#assert 0==1?\n",
        "\n",
        "        if origin_path!=None:\n",
        "            origin_df=pd.read_csv(origin_path)\n",
        "            print(f\"len(origin_df):{len(origin_df)}\")\n",
        "\n",
        "            self.train_df=pd.concat((self.train_df,origin_df),axis=0)\n",
        "            print(f\"len(self.train_df):{len(self.train_df)}\")\n",
        "        self.train_df.fillna(method=\"ffill\",inplace=True)\n",
        "        self.train_df=self.train_df.drop_duplicates()\n",
        "        print(f\"len(self.train_df):{len(self.train_df)}\")\n",
        "\n",
        "        if test_path!=None:\n",
        "            self.test_df=pd.read_csv(test_path)\n",
        "            self.test_df.drop(['id'],axis=1,inplace=True)\n",
        "            self.test_df.fillna(method=\"ffill\",inplace=True)\n",
        "        else:\n",
        "            assert 0#assert 0==1?\n",
        "\n",
        "        self.num_cols=[]\n",
        "\n",
        "        self.cat_cols=[]\n",
        "\n",
        "        self.str_cols=[]\n",
        "        for col in self.train_df.columns:\n",
        "            if (self.train_df[col].dtype!='float') and (col!=self.TARGET_NAME) and (self.train_df[col].nunique()<50):\n",
        "                self.cat_cols.append(col)\n",
        "            elif (self.train_df[col].dtype=='float') and (col!=self.TARGET_NAME):\n",
        "                self.num_cols.append(col)\n",
        "            elif (col!=self.TARGET_NAME):\n",
        "                self.str_cols.append(col)\n",
        "        print(f\"num_cols:{self.num_cols}.\\ncat_cols:{self.cat_cols}.\\nstr_cols:{self.str_cols} \")\n",
        "\n",
        "\n",
        "    def pearson_corr(self,x1,x2):\n",
        "        #x1,x2 np.array\n",
        "        eps=1e-15\n",
        "        mean_x1=np.mean(x1)\n",
        "        mean_x2=np.mean(x2)\n",
        "        std_x1=np.std(x1)\n",
        "        std_x2=np.std(x2)\n",
        "        pearson=np.mean((x1-mean_x1)*(x2-mean_x2))/(std_x1*std_x2+eps)\n",
        "        return pearson\n",
        "\n",
        "\n",
        "    def EDA(self,):\n",
        "        print(\"num_cols VS cat_cols:\")\n",
        "        for num_col in self.num_cols:\n",
        "            for cat_col in self.cat_cols:\n",
        "                unique_value=self.train_df[cat_col].unique()\n",
        "                for value in unique_value:\n",
        "                    tmp_df=self.train_df[self.train_df[cat_col]==value]\n",
        "                    print(f\"{cat_col}=={value}:mean_{num_col}=={tmp_df[num_col].mean()}\")\n",
        "                print(\"-\"*50)\n",
        "            print()\n",
        "        print(\"num_cols VS num_cols:\")\n",
        "        for i in range(len(self.num_cols)):\n",
        "            for j in range(i+1,len(self.num_cols)):\n",
        "                x1=self.train_df[self.num_cols[i]].values\n",
        "                x2=self.train_df[self.num_cols[j]].values\n",
        "                if abs(self.pearson_corr(x1,x2))>0.9:\n",
        "                    print(f\"{self.num_cols[i]} and {self.num_cols[j]} have strong linear correlation.\")\n",
        "\n",
        "\n",
        "        if self.train_df[self.TARGET_NAME].nunique()<50:\n",
        "            print(\"TARGET VS cat_cols:\")\n",
        "            unique_target=self.train_df[self.TARGET_NAME].unique()\n",
        "            for target in unique_target:\n",
        "                for cat_col in self.cat_cols:\n",
        "                    unique_value=self.train_df[cat_col].unique()\n",
        "                    for value in unique_value:\n",
        "\n",
        "                        tmp_df=self.train_df[self.train_df[cat_col]==value]\n",
        "\n",
        "                        if len(tmp_df[tmp_df[self.TARGET_NAME]==target])>0.99*len(tmp_df):\n",
        "                            print(f\"when {cat_col}={value},{self.TARGET_NAME}={target}\")\n",
        "\n",
        "                        tmp_df=self.train_df[self.train_df[self.TARGET_NAME]==target]\n",
        "                        if len(tmp_df[tmp_df[cat_col]==value])>0.99*len(tmp_df):\n",
        "                            print(f\"when {self.TARGET_NAME}={target},{cat_col}={value}\")\n",
        "            print(\"-\"*50)\n",
        "            print(\"TARGET VS num_cols:\")\n",
        "            for num_col in self.num_cols:\n",
        "                for target in unique_target:\n",
        "                    tmp_df=self.train_df[self.train_df[self.TARGET_NAME]==target]\n",
        "                    print(f\"when {self.TARGET_NAME}={target},mean_{num_col}={tmp_df[num_col].mean()}\")\n",
        "                print(\"-\"*50)\n",
        "        else:\n",
        "            print(\"TARGET VS cat_cols:\")\n",
        "            for target in unique_target:\n",
        "                for cat_col in self.cat_cols:\n",
        "                    unique_value=self.train_df[cat_col].unique()\n",
        "                    for value in unique_value:\n",
        "                        tmp_df=self.train_df[self.train_df[cat_col]==value]\n",
        "                        print(f\"when {cat_col}={value},mean_{self.TARGET_NAME}={tmp_df[self.TARGET_NAME].mean()}\")\n",
        "                print(\"-\"*50)\n",
        "\n",
        "\n",
        "\n",
        "    def tf_idf(self,train, test, column,n,p):\n",
        "        print(f\"tf-idf done with {column}\")\n",
        "        vectorizer=TfidfVectorizer(max_features=n)\n",
        "\n",
        "        vectors_train=vectorizer.fit_transform(train[column])\n",
        "        vectors_test=vectorizer.transform(test[column])\n",
        "\n",
        "        svd=TruncatedSVD(p)\n",
        "        x_pca_train=svd.fit_transform(vectors_train)\n",
        "        x_pca_test=svd.transform(vectors_test)\n",
        "        tfidf_df_train=pd.DataFrame(x_pca_train)\n",
        "        print(len(tfidf_df_train))\n",
        "        tfidf_df_test=pd.DataFrame(x_pca_test)\n",
        "\n",
        "        cols=[(column+\"_tfidf_\"+str(f)) for f in tfidf_df_train.columns]\n",
        "        tfidf_df_train.columns=cols\n",
        "        tfidf_df_test.columns=cols\n",
        "\n",
        "        train=pd.concat([train,tfidf_df_train], axis=1)\n",
        "        test=pd.concat([test,tfidf_df_test], axis=1)\n",
        "        train.drop([column],axis=1,inplace=True)\n",
        "        test.drop([column],axis=1,inplace=True)\n",
        "        return train, test\n",
        "\n",
        "    def one_hot_encoder(self,total_df):\n",
        "        print(f\"one hot encoder with {self.cat_cols}\")\n",
        "        for col in self.cat_cols:\n",
        "            if total_df[col].nunique()==2:\n",
        "                value=total_df[col].values[0]\n",
        "                total_df[col+\"_\"+str(value)]=(total_df[col]==value)\n",
        "                total_df.drop([col],axis=1,inplace=True)\n",
        "            else:\n",
        "                values=total_df[col].unique()\n",
        "                for value in values:\n",
        "                    total_df[col+\"_\"+str(value)]=(total_df[col]==value)\n",
        "                total_df.drop([col],axis=1,inplace=True)\n",
        "        return total_df\n",
        "\n",
        "    def make_feats(self):\n",
        "        total_df=pd.concat((self.train_df,self.test_df),axis=0)\n",
        "        print(f\"len(total_df):{len(total_df)}\")\n",
        "        total_df=self.one_hot_encoder(total_df)\n",
        "\n",
        "#         total_df['Age']=round(total_df['Age'],2)\n",
        "#         total_df['Height']=round(total_df['Height'],2)\n",
        "#         total_df['Weight']=round(total_df['Weight'],2)\n",
        "#         total_df['BMI']=total_df['Weight']/(total_df['Height']**2)\n",
        "#         cols_to_round = ['FCVC',\"NCP\",\"CH2O\",\"FAF\",\"TUE\"]\n",
        "#         for col in cols_to_round:\n",
        "#             total_df[col] = round(total_df[col])\n",
        "#             total_df[col] = total_df[col].astype('int')\n",
        "\n",
        "\n",
        "        print(\"-\"*50)\n",
        "        train_feats=total_df[:len(self.train_df)].reset_index(drop=True)\n",
        "        test_feats=total_df[len(self.train_df):].reset_index(drop=True)\n",
        "        print(len(train_feats))\n",
        "        for col in self.str_cols:\n",
        "            train_feats,test_feats=self.tf_idf(train_feats, test_feats, column=col,n=int(0.5*train_feats[col].nunique()),p=5)\n",
        "            print(f\"len(train_feats):{len(train_feats)}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Done!! total_features:{len(train_feats.keys().values)-1}\")\n",
        "        return train_feats,test_feats\n",
        "\n",
        "\n",
        "    def submission(self,submission_path=None,test_pred=None):\n",
        "        if submission_path!=None:\n",
        "            self.submission_df=pd.read_csv(submission_path)\n",
        "        else:\n",
        "            assert 0#assert 0==1?\n",
        "        self.submission_df[self.TARGET_NAME]=test_pred\n",
        "        self.submission_df.to_csv(\"submission.csv\",index=None)\n",
        "        return self.submission_df"
      ],
      "metadata": {
        "trusted": true,
        "id": "b11Y7BWlyM9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineer"
      ],
      "metadata": {
        "id": "noxmQqZLyM9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Import data.\")\n",
        "preprocessor=Preprocessor(Config.train_path,Config.origin_path,Config.test_path,Config.TARGET_NAME,Config.seed)\n",
        "print(\"-\"*50)\n",
        "print(\"EDA!!!\")\n",
        "preprocessor.EDA()\n",
        "print(\"-\"*50)\n",
        "print(\"make features.\")\n",
        "train_feats,test_feats=preprocessor.make_feats()\n",
        "print(\"-\"*50)\n",
        "print(\"labels to idx\")\n",
        "labels=['Insufficient_Weight','Normal_Weight', 'Overweight_Level_I','Overweight_Level_II','Obesity_Type_I','Obesity_Type_II', 'Obesity_Type_III']\n",
        "labels2idx={}\n",
        "idx2labels={}\n",
        "for idx in range(len(labels)):\n",
        "    labels2idx[labels[idx]]=idx\n",
        "    idx2labels[idx]=labels[idx]\n",
        "train_feats['NObeyesdad']=train_feats['NObeyesdad'].apply(lambda x: labels2idx[x])"
      ],
      "metadata": {
        "trusted": true,
        "id": "U6BqQqN6yM9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "EXxrVDugyM9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def fit_and_predict(model):\n",
        "    X=train_feats.drop([Config.TARGET_NAME],axis=1).copy()\n",
        "    y=train_feats[Config.TARGET_NAME].copy()\n",
        "    test_X=test_feats.drop([Config.TARGET_NAME],axis=1).copy()\n",
        "    oof_pred_pro=np.zeros((len(X),np.max(y)+1))\n",
        "    test_pred_pro=np.zeros((Config.num_folds,len(test_X),np.max(y)+1))\n",
        "    #10\n",
        "    skf = StratifiedKFold(n_splits=Config.num_folds,random_state=Config.seed, shuffle=True)\n",
        "\n",
        "    for fold, (train_index, valid_index) in (enumerate(skf.split(X, y.astype(str)))):\n",
        "        print(f\"fold:{fold}\")\n",
        "\n",
        "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
        "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "\n",
        "        model.fit(X_train,y_train)\n",
        "\n",
        "        oof_pred_pro[valid_index]=model.predict_proba(X_valid)\n",
        "        test_pred_pro[fold]=model.predict_proba(test_X)\n",
        "\n",
        "    return oof_pred_pro,test_pred_pro\n",
        "\n",
        "lgb_params={\n",
        "    \"objective\": \"multiclass\",          # Objective function for the model\n",
        "    \"metric\": \"multi_logloss\",          # Evaluation metric\n",
        "    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n",
        "    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n",
        "    \"random_state\": Config.seed,       # Random state for reproducibility\n",
        "    \"num_class\": 7,                     # Number of classes in the dataset\n",
        "    'learning_rate': 0.030962211546832760,  # Learning rate for gradient boosting\n",
        "    'n_estimators': 500,                # Number of boosting iterations\n",
        "    'lambda_l1': 0.009667446568254372,  # L1 regularization term\n",
        "    'lambda_l2': 0.04018641437301800,   # L2 regularization term\n",
        "    'max_depth': 10,                    # Maximum depth of the trees\n",
        "    'colsample_bytree': 0.40977129346872643,  # Fraction of features to consider for each tree\n",
        "    'subsample': 0.9535797422450176,    # Fraction of samples to consider for each boosting iteration\n",
        "    'min_child_samples': 26             # Minimum number of data needed in a leaf\n",
        "}\n",
        "\n",
        "xgb_params = {'grow_policy': 'depthwise', 'n_estimators': 982,\n",
        "               'learning_rate': 0.050053726931263504, 'gamma': 0.5354391952653927,\n",
        "               'subsample': 0.7060590452456204, 'colsample_bytree': 0.37939433412123275,\n",
        "               'max_depth': 23, 'min_child_weight': 21, 'reg_lambda': 9.150224029846654e-08,\n",
        "               'reg_alpha': 5.671063656994295e-08,\n",
        "               'booster':'gbtree','objective':'multi:softmax',\"verbosity\":0\n",
        "              }\n",
        "cat_params={'learning_rate': 0.13762007048684638, 'depth': 5,\n",
        "          'l2_leaf_reg': 5.285199432056192, 'bagging_temperature': 0.6029582154263095,\n",
        "         'random_seed': Config.seed,'verbose': False,'iterations':1000}\n",
        "\n",
        "print(\"random forest model\")\n",
        "rf_oof_pred_pro,rf_test_pred_pro=fit_and_predict(model=RandomForestClassifier(random_state=Config.seed))\n",
        "print(\"lgb model\")\n",
        "lgb_oof_pred_pro,lgb_test_pred_pro=fit_and_predict(model= LGBMClassifier(**lgb_params,verbose=-1))\n",
        "print(\"xgb model\")\n",
        "xgb_oof_pred_pro,xgb_test_pred_pro=fit_and_predict(model= XGBClassifier(**xgb_params,seed=Config.seed))\n",
        "print(\"cat model\")\n",
        "cat_oof_pred_pro,cat_test_pred_pro=fit_and_predict(model= CatBoostClassifier(**cat_params))"
      ],
      "metadata": {
        "trusted": true,
        "id": "CrjGkNebyM9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blending"
      ],
      "metadata": {
        "id": "0wndpffvyM9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true,y_pred):\n",
        "    eps=10**(-15)\n",
        "    y_true=np.clip(y_true,eps,1-eps)\n",
        "    y_pred=np.clip(y_pred,eps,1-eps)\n",
        "    return -np.mean(np.sum(y_true*np.log(y_pred),axis=-1))\n",
        "def accuracy(y_true,y_pred):\n",
        "    return np.mean(y_true==y_pred)\n",
        "oof_target=train_feats[Config.TARGET_NAME].values\n",
        "oof_one_hot=np.eye(np.max(oof_target)+1)[oof_target]\n",
        "step=100\n",
        "best_w1=1\n",
        "best_w2=1\n",
        "best_w3=1\n",
        "best_accuracy=0\n",
        "best_log_loss=10\n",
        "for w1 in range(1,step-1,1):\n",
        "    for w2 in range(1,step-w1-1,1):\n",
        "        for w3 in range(1,step-w1-w2-1,1):\n",
        "            blend_oof_pred_pro=(w1*rf_oof_pred_pro+w2*lgb_oof_pred_pro+w3*xgb_oof_pred_pro+(step-w1-w2-w3)*cat_oof_pred_pro)/step\n",
        "            current_log_loss=log_loss(oof_one_hot,blend_oof_pred_pro)\n",
        "            current_accuracy=accuracy(oof_target,np.argmax(blend_oof_pred_pro,axis=1))\n",
        "            if current_accuracy>best_accuracy:\n",
        "                best_w1=w1\n",
        "                best_w2=w2\n",
        "                best_w3=w3\n",
        "                best_accuracy=current_accuracy\n",
        "                best_log_loss=current_log_loss\n",
        "                print(f\"best_w1:{best_w1},best_w2:{best_w2},best_w3:{best_w3},best_accuracy:{best_accuracy},best_log_loss:{best_log_loss}\")\n",
        "            elif (current_accuracy==best_accuracy) and current_log_loss<best_log_loss:\n",
        "                best_w1=w1\n",
        "                best_w2=w2\n",
        "                best_w3=w3\n",
        "                best_log_loss=current_log_loss\n",
        "                print(f\"best_w1:{best_w1},best_w2:{best_w2},best_w3:{best_w3},best_accuracy:{best_accuracy},best_log_loss:{best_log_loss}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "jOZps3kwyM9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission"
      ],
      "metadata": {
        "id": "UXup3M2XyM9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blend_test_pred_pro=(best_w1*rf_test_pred_pro+best_w2*lgb_test_pred_pro+best_w3*xgb_test_pred_pro+(step-best_w1-best_w2-best_w3)*cat_test_pred_pro)/step\n",
        "test_pred=np.argmax(blend_test_pred_pro.mean(axis=0),axis=1)\n",
        "test_pred=[idx2labels[int(pred)] for pred in test_pred]\n",
        "print(\"submission\")\n",
        "submission_df=preprocessor.submission(submission_path=Config.submission_path,test_pred=test_pred)\n",
        "submission_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vWUrDVbKyM9Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}