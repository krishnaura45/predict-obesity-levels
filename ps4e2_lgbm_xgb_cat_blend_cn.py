# -*- coding: utf-8 -*-
"""pss4e2 study note

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/krishd123/pss4e2-study-note.7a22d7b2-af1c-44b0-8b65-b7503800fcb8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250421/auto/storage/goog4_request%26X-Goog-Date%3D20250421T095814Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D25b2932396314f663c0b2ee88aed49314ff215b65922b894e2a4df530579b89bd933de0900ba9ebf2d87bcafb4b513615d103752e6a7c4cc0fe901fb6339ee172e93f6c1b3611bedd71a814cf7afcc998fcc08d504c6e50f93edca0a476fb10c654871adefffa1bca7cdb3109f80e27e39956bee75917a90130861ddfc61abb26507a9bef624a59f80f026b63d989db9ab0758563b8612abceadac959620ab118fda859c0659ccd4f2e505fdceaa4317ba6bc7f30c779e869aa329400c1466367915641641638a62303ec084fdf2bcea6c4d4e2c81cd78e893273f8db21232efb07fb39c2af0dba453320b15388ed7d59823988fbe3e7e3007e8ec08aeb83361
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

playground_series_s4e2_path = kagglehub.competition_download('playground-series-s4e2')
aravindpcoder_obesity_or_cvd_risk_classifyregressorcluster_path = kagglehub.dataset_download('aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster')

print('Data source import complete.')

"""### Created by Cosmic Black

#### This notebook is studied from <a href="https://www.kaggle.com/code/ksevta/ps4e2-xgb-lgbm-0-92#Feature-Engineering-&-Processing">PS4E2: XGB / LGBM | 0.92+</a>. If you think my notebook is useful, don't forget to share it with all.

## Import necessary libraries

Setting up the notebook by importing essential libraries for data handling, text processing, dimensionality reduction, random number generation, and warning management
"""

#necessary
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import random
import warnings
warnings.filterwarnings('ignore') #to suppress warning messages

"""## Config"""

#config
class Config():
    origin_path="/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv"
    train_path='/kaggle/input/playground-series-s4e2/train.csv'
    test_path='/kaggle/input/playground-series-s4e2/test.csv'
    submission_path='/kaggle/input/playground-series-s4e2/sample_submission.csv'
    seed=2024
    num_folds=10
    TARGET_NAME ='NObeyesdad'

"""## preprocessor"""

class Preprocessor:
    def __init__(self,train_path=None,origin_path=None,test_path=None,TARGET_NAME ='target',seed=2024):
        self.seed=seed

        np.random.seed(self.seed)
        random.seed(self.seed)

        self.TARGET_NAME=TARGET_NAME

        if train_path!=None:
            self.train_df=pd.read_csv(train_path)
            self.train_df.drop(['id'],axis=1,inplace=True)
        else:
            assert 0#assert 0==1?

        if origin_path!=None:
            origin_df=pd.read_csv(origin_path)
            print(f"len(origin_df):{len(origin_df)}")

            self.train_df=pd.concat((self.train_df,origin_df),axis=0)
            print(f"len(self.train_df):{len(self.train_df)}")
        self.train_df.fillna(method="ffill",inplace=True)
        self.train_df=self.train_df.drop_duplicates()
        print(f"len(self.train_df):{len(self.train_df)}")

        if test_path!=None:
            self.test_df=pd.read_csv(test_path)
            self.test_df.drop(['id'],axis=1,inplace=True)
            self.test_df.fillna(method="ffill",inplace=True)
        else:
            assert 0#assert 0==1?

        self.num_cols=[]

        self.cat_cols=[]

        self.str_cols=[]
        for col in self.train_df.columns:
            if (self.train_df[col].dtype!='float') and (col!=self.TARGET_NAME) and (self.train_df[col].nunique()<50):
                self.cat_cols.append(col)
            elif (self.train_df[col].dtype=='float') and (col!=self.TARGET_NAME):
                self.num_cols.append(col)
            elif (col!=self.TARGET_NAME):
                self.str_cols.append(col)
        print(f"num_cols:{self.num_cols}.\ncat_cols:{self.cat_cols}.\nstr_cols:{self.str_cols} ")


    def pearson_corr(self,x1,x2):
        #x1,x2 np.array
        eps=1e-15
        mean_x1=np.mean(x1)
        mean_x2=np.mean(x2)
        std_x1=np.std(x1)
        std_x2=np.std(x2)
        pearson=np.mean((x1-mean_x1)*(x2-mean_x2))/(std_x1*std_x2+eps)
        return pearson


    def EDA(self,):
        print("num_cols VS cat_cols:")
        for num_col in self.num_cols:
            for cat_col in self.cat_cols:
                unique_value=self.train_df[cat_col].unique()
                for value in unique_value:
                    tmp_df=self.train_df[self.train_df[cat_col]==value]
                    print(f"{cat_col}=={value}:mean_{num_col}=={tmp_df[num_col].mean()}")
                print("-"*50)
            print()
        print("num_cols VS num_cols:")
        for i in range(len(self.num_cols)):
            for j in range(i+1,len(self.num_cols)):
                x1=self.train_df[self.num_cols[i]].values
                x2=self.train_df[self.num_cols[j]].values
                if abs(self.pearson_corr(x1,x2))>0.9:
                    print(f"{self.num_cols[i]} and {self.num_cols[j]} have strong linear correlation.")


        if self.train_df[self.TARGET_NAME].nunique()<50:
            print("TARGET VS cat_cols:")
            unique_target=self.train_df[self.TARGET_NAME].unique()
            for target in unique_target:
                for cat_col in self.cat_cols:
                    unique_value=self.train_df[cat_col].unique()
                    for value in unique_value:

                        tmp_df=self.train_df[self.train_df[cat_col]==value]

                        if len(tmp_df[tmp_df[self.TARGET_NAME]==target])>0.99*len(tmp_df):
                            print(f"when {cat_col}={value},{self.TARGET_NAME}={target}")

                        tmp_df=self.train_df[self.train_df[self.TARGET_NAME]==target]
                        if len(tmp_df[tmp_df[cat_col]==value])>0.99*len(tmp_df):
                            print(f"when {self.TARGET_NAME}={target},{cat_col}={value}")
            print("-"*50)
            print("TARGET VS num_cols:")
            for num_col in self.num_cols:
                for target in unique_target:
                    tmp_df=self.train_df[self.train_df[self.TARGET_NAME]==target]
                    print(f"when {self.TARGET_NAME}={target},mean_{num_col}={tmp_df[num_col].mean()}")
                print("-"*50)
        else:
            print("TARGET VS cat_cols:")
            for target in unique_target:
                for cat_col in self.cat_cols:
                    unique_value=self.train_df[cat_col].unique()
                    for value in unique_value:
                        tmp_df=self.train_df[self.train_df[cat_col]==value]
                        print(f"when {cat_col}={value},mean_{self.TARGET_NAME}={tmp_df[self.TARGET_NAME].mean()}")
                print("-"*50)



    def tf_idf(self,train, test, column,n,p):
        print(f"tf-idf done with {column}")
        vectorizer=TfidfVectorizer(max_features=n)

        vectors_train=vectorizer.fit_transform(train[column])
        vectors_test=vectorizer.transform(test[column])

        svd=TruncatedSVD(p)
        x_pca_train=svd.fit_transform(vectors_train)
        x_pca_test=svd.transform(vectors_test)
        tfidf_df_train=pd.DataFrame(x_pca_train)
        print(len(tfidf_df_train))
        tfidf_df_test=pd.DataFrame(x_pca_test)

        cols=[(column+"_tfidf_"+str(f)) for f in tfidf_df_train.columns]
        tfidf_df_train.columns=cols
        tfidf_df_test.columns=cols

        train=pd.concat([train,tfidf_df_train], axis=1)
        test=pd.concat([test,tfidf_df_test], axis=1)
        train.drop([column],axis=1,inplace=True)
        test.drop([column],axis=1,inplace=True)
        return train, test

    def one_hot_encoder(self,total_df):
        print(f"one hot encoder with {self.cat_cols}")
        for col in self.cat_cols:
            if total_df[col].nunique()==2:
                value=total_df[col].values[0]
                total_df[col+"_"+str(value)]=(total_df[col]==value)
                total_df.drop([col],axis=1,inplace=True)
            else:
                values=total_df[col].unique()
                for value in values:
                    total_df[col+"_"+str(value)]=(total_df[col]==value)
                total_df.drop([col],axis=1,inplace=True)
        return total_df

    def make_feats(self):
        total_df=pd.concat((self.train_df,self.test_df),axis=0)
        print(f"len(total_df):{len(total_df)}")
        total_df=self.one_hot_encoder(total_df)

#         total_df['Age']=round(total_df['Age'],2)
#         total_df['Height']=round(total_df['Height'],2)
#         total_df['Weight']=round(total_df['Weight'],2)
#         total_df['BMI']=total_df['Weight']/(total_df['Height']**2)
#         cols_to_round = ['FCVC',"NCP","CH2O","FAF","TUE"]
#         for col in cols_to_round:
#             total_df[col] = round(total_df[col])
#             total_df[col] = total_df[col].astype('int')


        print("-"*50)
        train_feats=total_df[:len(self.train_df)].reset_index(drop=True)
        test_feats=total_df[len(self.train_df):].reset_index(drop=True)
        print(len(train_feats))
        for col in self.str_cols:
            train_feats,test_feats=self.tf_idf(train_feats, test_feats, column=col,n=int(0.5*train_feats[col].nunique()),p=5)
            print(f"len(train_feats):{len(train_feats)}")
        print("-"*50)
        print(f"Done!! total_features:{len(train_feats.keys().values)-1}")
        return train_feats,test_feats


    def submission(self,submission_path=None,test_pred=None):
        if submission_path!=None:
            self.submission_df=pd.read_csv(submission_path)
        else:
            assert 0#assert 0==1?
        self.submission_df[self.TARGET_NAME]=test_pred
        self.submission_df.to_csv("submission.csv",index=None)
        return self.submission_df

"""### Feature engineer"""

print("Import data.")
preprocessor=Preprocessor(Config.train_path,Config.origin_path,Config.test_path,Config.TARGET_NAME,Config.seed)
print("-"*50)
print("EDA!!!")
preprocessor.EDA()
print("-"*50)
print("make features.")
train_feats,test_feats=preprocessor.make_feats()
print("-"*50)
print("labels to idx")
labels=['Insufficient_Weight','Normal_Weight', 'Overweight_Level_I','Overweight_Level_II','Obesity_Type_I','Obesity_Type_II', 'Obesity_Type_III']
labels2idx={}
idx2labels={}
for idx in range(len(labels)):
    labels2idx[labels[idx]]=idx
    idx2labels[idx]=labels[idx]
train_feats['NObeyesdad']=train_feats['NObeyesdad'].apply(lambda x: labels2idx[x])

"""## Model training"""

#model
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier

#KFold
from sklearn.model_selection import StratifiedKFold

def fit_and_predict(model):
    X=train_feats.drop([Config.TARGET_NAME],axis=1).copy()
    y=train_feats[Config.TARGET_NAME].copy()
    test_X=test_feats.drop([Config.TARGET_NAME],axis=1).copy()
    oof_pred_pro=np.zeros((len(X),np.max(y)+1))
    test_pred_pro=np.zeros((Config.num_folds,len(test_X),np.max(y)+1))
    #10
    skf = StratifiedKFold(n_splits=Config.num_folds,random_state=Config.seed, shuffle=True)

    for fold, (train_index, valid_index) in (enumerate(skf.split(X, y.astype(str)))):
        print(f"fold:{fold}")

        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]

        model.fit(X_train,y_train)

        oof_pred_pro[valid_index]=model.predict_proba(X_valid)
        test_pred_pro[fold]=model.predict_proba(test_X)

    return oof_pred_pro,test_pred_pro

lgb_params={
    "objective": "multiclass",          # Objective function for the model
    "metric": "multi_logloss",          # Evaluation metric
    "verbosity": -1,                    # Verbosity level (-1 for silent)
    "boosting_type": "gbdt",            # Gradient boosting type
    "random_state": Config.seed,       # Random state for reproducibility
    "num_class": 7,                     # Number of classes in the dataset
    'learning_rate': 0.030962211546832760,  # Learning rate for gradient boosting
    'n_estimators': 500,                # Number of boosting iterations
    'lambda_l1': 0.009667446568254372,  # L1 regularization term
    'lambda_l2': 0.04018641437301800,   # L2 regularization term
    'max_depth': 10,                    # Maximum depth of the trees
    'colsample_bytree': 0.40977129346872643,  # Fraction of features to consider for each tree
    'subsample': 0.9535797422450176,    # Fraction of samples to consider for each boosting iteration
    'min_child_samples': 26             # Minimum number of data needed in a leaf
}

xgb_params = {'grow_policy': 'depthwise', 'n_estimators': 982,
               'learning_rate': 0.050053726931263504, 'gamma': 0.5354391952653927,
               'subsample': 0.7060590452456204, 'colsample_bytree': 0.37939433412123275,
               'max_depth': 23, 'min_child_weight': 21, 'reg_lambda': 9.150224029846654e-08,
               'reg_alpha': 5.671063656994295e-08,
               'booster':'gbtree','objective':'multi:softmax',"verbosity":0
              }
cat_params={'learning_rate': 0.13762007048684638, 'depth': 5,
          'l2_leaf_reg': 5.285199432056192, 'bagging_temperature': 0.6029582154263095,
         'random_seed': Config.seed,'verbose': False,'iterations':1000}

print("random forest model")
rf_oof_pred_pro,rf_test_pred_pro=fit_and_predict(model=RandomForestClassifier(random_state=Config.seed))
print("lgb model")
lgb_oof_pred_pro,lgb_test_pred_pro=fit_and_predict(model= LGBMClassifier(**lgb_params,verbose=-1))
print("xgb model")
xgb_oof_pred_pro,xgb_test_pred_pro=fit_and_predict(model= XGBClassifier(**xgb_params,seed=Config.seed))
print("cat model")
cat_oof_pred_pro,cat_test_pred_pro=fit_and_predict(model= CatBoostClassifier(**cat_params))

"""## Blending"""

def log_loss(y_true,y_pred):
    eps=10**(-15)
    y_true=np.clip(y_true,eps,1-eps)
    y_pred=np.clip(y_pred,eps,1-eps)
    return -np.mean(np.sum(y_true*np.log(y_pred),axis=-1))
def accuracy(y_true,y_pred):
    return np.mean(y_true==y_pred)
oof_target=train_feats[Config.TARGET_NAME].values
oof_one_hot=np.eye(np.max(oof_target)+1)[oof_target]
step=100
best_w1=1
best_w2=1
best_w3=1
best_accuracy=0
best_log_loss=10
for w1 in range(1,step-1,1):
    for w2 in range(1,step-w1-1,1):
        for w3 in range(1,step-w1-w2-1,1):
            blend_oof_pred_pro=(w1*rf_oof_pred_pro+w2*lgb_oof_pred_pro+w3*xgb_oof_pred_pro+(step-w1-w2-w3)*cat_oof_pred_pro)/step
            current_log_loss=log_loss(oof_one_hot,blend_oof_pred_pro)
            current_accuracy=accuracy(oof_target,np.argmax(blend_oof_pred_pro,axis=1))
            if current_accuracy>best_accuracy:
                best_w1=w1
                best_w2=w2
                best_w3=w3
                best_accuracy=current_accuracy
                best_log_loss=current_log_loss
                print(f"best_w1:{best_w1},best_w2:{best_w2},best_w3:{best_w3},best_accuracy:{best_accuracy},best_log_loss:{best_log_loss}")
            elif (current_accuracy==best_accuracy) and current_log_loss<best_log_loss:
                best_w1=w1
                best_w2=w2
                best_w3=w3
                best_log_loss=current_log_loss
                print(f"best_w1:{best_w1},best_w2:{best_w2},best_w3:{best_w3},best_accuracy:{best_accuracy},best_log_loss:{best_log_loss}")

"""## Submission"""

blend_test_pred_pro=(best_w1*rf_test_pred_pro+best_w2*lgb_test_pred_pro+best_w3*xgb_test_pred_pro+(step-best_w1-best_w2-best_w3)*cat_test_pred_pro)/step
test_pred=np.argmax(blend_test_pred_pro.mean(axis=0),axis=1)
test_pred=[idx2labels[int(pred)] for pred in test_pred]
print("submission")
submission_df=preprocessor.submission(submission_path=Config.submission_path,test_pred=test_pred)
submission_df.head()